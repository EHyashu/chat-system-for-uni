"""
VIVA-READY EXPLANATION: UPGRADED RAG SYSTEM
===========================================

This document provides clear, simple explanations of all intelligent features
and evaluation metrics for university project presentation and viva defense.

---
PART 1: INTELLIGENT FEATURES (HOW WE MADE IT SMARTER)
---

1. MAXIMAL MARGINAL RELEVANCE (MMR)
   
   What it does:
   - Retrieves diverse, non-redundant information
   - Avoids showing the same content multiple times
   
   How it works:
   - First, retrieve 15 candidate documents (top_k * 3)
   - Then select 5 final documents that are:
     * Relevant to the query (similarity score high)
     * Diverse from each other (not too similar to already selected docs)
   
   Formula:
     MMR_score = Î» Ã— relevance + (1-Î») Ã— diversity
     where Î» = 0.7 (favor relevance over diversity)
   
   Example:
     Query: "What is the attendance policy?"
     Without MMR: Might retrieve 5 chunks all saying "75% attendance required"
     With MMR: Retrieves attendance rule + consequences + exemptions + appeals


2. CONFIDENCE SCORING
   
   What it does:
   - Assigns a confidence score (0-1) to each answer
   - Helps users trust or question the response
   
   How it's calculated:
     Confidence = 0.5 Ã— top_retrieval_score
                + 0.3 Ã— avg_retrieval_score
                + 0.2 Ã— chunk_agreement
   
   Components:
   a) Top score: Best match quality (how relevant is the top result?)
   b) Average score: Overall relevance (are all results good?)
   c) Chunk agreement: Consistency (do retrieved chunks agree with each other?)
   
   Example:
     High confidence (0.85): All chunks highly relevant and consistent
     Low confidence (0.30): Weak matches, contradictory information


3. CONTEXT VALIDATION
   
   What it does:
   - Refuses to answer when confidence < 0.25
   - Prevents low-quality hallucinated responses
   
   How it works:
   - If retrieval confidence < threshold â†’ return:
     "I could not find this information in the university documents."
   - This is BETTER than generating a weak/wrong answer
   
   Example:
     Query: "What is the hostel WiFi password?"
     If no relevant docs found â†’ System refuses instead of guessing


4. ENHANCED PROMPT ENGINEERING
   
   What it does:
   - Forces the LLM to follow strict rules
   - Requires step-by-step reasoning
   - Demands citation of sources
   
   Key instructions to LLM:
   1. Answer ONLY from context
   2. Use step-by-step reasoning for complex queries
   3. Cite document names
   4. Use bullet points for lists
   5. Never assume or add external knowledge
   6. Refuse if context insufficient
   
   Example prompt structure:
     "CONTEXT: [retrieved documents]
      QUESTION: [user query]
      INSTRUCTIONS: [strict rules]
      YOUR ANSWER:"


5. ANSWER JUSTIFICATION & TRANSPARENCY
   
   What it provides:
   - Which documents were consulted
   - How many chunks were retrieved
   - Overall confidence score
   - Reasoning behind the answer
   
   Example output:
     Answer: "Minimum 75% attendance required..."
     
     Reasoning: "Retrieved 5 relevant document chunks.
                 Documents consulted: AttendanceRules.pdf, ExamPolicy.pdf.
                 Overall confidence: 0.87 (based on retrieval scores and chunk consistency)."


---
PART 2: EVALUATION METRICS (HOW WE MEASURE ACCURACY)
---

IMPORTANT: This is NOT a classification task.
We measure RAG quality through:
  1. Retrieval accuracy (did we find the right documents?)
  2. Answer quality (is the generated answer correct and grounded?)

---
A. RETRIEVAL METRICS
---

1. PRECISION@K
   
   Definition:
     Out of K retrieved documents, how many are actually relevant?
   
   Formula:
     Precision@K = (# relevant docs in top-K) / K
   
   Example:
     Query: "What is the DBMS syllabus?"
     Retrieved 5 docs: [DBMS_Syllabus.pdf, CSE_Handbook.pdf, TimeTable.pdf, 
                        DatabaseNotes.pdf, HostelRules.pdf]
     Relevant docs: {DBMS_Syllabus.pdf, DatabaseNotes.pdf}
     
     Precision@5 = 2/5 = 0.40 (40%)
   
   Interpretation:
     Higher is better. 1.0 = all retrieved docs are relevant.


2. RECALL@K
   
   Definition:
     Out of all relevant documents, how many did we retrieve in top-K?
   
   Formula:
     Recall@K = (# relevant docs in top-K) / (total # relevant docs)
   
   Example:
     Same query, total relevant docs in database = 3
     (DBMS_Syllabus.pdf, DatabaseNotes.pdf, DBMS_Lab.pdf)
     
     Retrieved 2 out of 3
     Recall@5 = 2/3 = 0.67 (67%)
   
   Interpretation:
     Higher is better. 1.0 = found all relevant docs.


3. MEAN RECIPROCAL RANK (MRR)
   
   Definition:
     On average, at what position does the first relevant document appear?
   
   Formula:
     For each query: reciprocal rank = 1 / (position of first relevant doc)
     MRR = average of all reciprocal ranks
   
   Example:
     Query 1: First relevant doc at position 1 â†’ RR = 1/1 = 1.0
     Query 2: First relevant doc at position 3 â†’ RR = 1/3 = 0.33
     Query 3: First relevant doc at position 2 â†’ RR = 1/2 = 0.50
     
     MRR = (1.0 + 0.33 + 0.50) / 3 = 0.61
   
   Interpretation:
     Higher is better. 1.0 = first result always relevant.


4. F1@K
   
   Definition:
     Harmonic mean of Precision@K and Recall@K
   
   Formula:
     F1@K = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
   
   Why useful:
     Balances precision and recall into one number


---
B. ANSWER QUALITY METRICS
---

1. SEMANTIC SIMILARITY
   
   Definition:
     How similar is the generated answer to the expected answer (in meaning)?
   
   How it works:
     - Convert both answers to embeddings (dense vectors)
     - Calculate cosine similarity
   
   Formula:
     similarity = cos(embedding_generated, embedding_expected)
                = (A Â· B) / (||A|| Ã— ||B||)
   
   Example:
     Expected: "Minimum 75% attendance required for exams"
     Generated: "Students need at least 75% attendance to sit for examinations"
     
     Semantic Similarity = 0.92 (very similar meaning)
   
   Interpretation:
     Score âˆˆ [0, 1], higher is better


2. FAITHFULNESS
   
   Definition:
     Is the answer grounded in the retrieved context?
     (Does the answer come from the documents or is it hallucinated?)
   
   How it works:
     - Embed the generated answer
     - Embed all retrieved context chunks
     - Calculate max similarity between answer and any chunk
   
   Formula:
     faithfulness = max(similarity(answer, chunk_i)) for all chunks
   
   Example:
     Answer: "75% attendance is required"
     Context chunks: ["Students must maintain 75% attendance...", 
                      "Examination eligibility requires...", ...]
     
     Faithfulness = 0.88 (answer strongly matches first chunk)
   
   Interpretation:
     High score (>0.7) = answer is grounded in context
     Low score (<0.5) = possible hallucination


3. ANSWER RELEVANCE
   
   Definition:
     How relevant is the answer to the original query?
   
   How it works:
     - Embed the query
     - Embed the answer
     - Calculate cosine similarity
   
   Example:
     Query: "What is the attendance policy?"
     Answer: "75% attendance required..."
     
     Answer Relevance = 0.85 (highly relevant)


4. EXACT MATCH
   
   Definition:
     Binary score: does generated answer exactly match expected answer?
   
   Formula:
     exact_match = 1.0 if generated == expected else 0.0
   
   Note:
     - Useful only when exact phrasing is required
     - Usually normalized (lowercase, strip whitespace)


---
C. HALLUCINATION DETECTION
---

What it does:
  Detects if the answer contains information NOT supported by retrieved context

How it works:
  1. Split answer into sentences
  2. For each sentence, check similarity to context chunks
  3. Flag sentences with similarity < threshold (0.3) as "unsupported"

Metrics:
  - is_hallucinated: True/False (any unsupported sentences?)
  - hallucination_score: 0-1 (higher = more hallucination)
  - unsupported_sentences: List of flagged sentences

Example:
  Context: "75% attendance required for exams"
  Answer: "75% attendance required. Students can also apply for medical exemption 
           by submitting a doctor's certificate."
  
  Sentence 1: "75% attendance required" â†’ similarity = 0.95 (OK)
  Sentence 2: "Students can also apply for medical exemption..." â†’ similarity = 0.15 (FLAGGED)
  
  Result:
    is_hallucinated = True
    hallucination_score = 0.40
    unsupported_sentences = ["Students can also apply for medical exemption..."]


---
D. AGGREGATE SCORE
---

Definition:
  Overall RAG system quality combining all metrics

Formula:
  aggregate_score = 0.25 Ã— retrieval_F1
                  + 0.25 Ã— semantic_similarity
                  + 0.30 Ã— faithfulness
                  + 0.20 Ã— (1 - hallucination_score)

Interpretation:
  Score âˆˆ [0, 1]
  - >0.8: Excellent
  - 0.6-0.8: Good
  - 0.4-0.6: Moderate (needs improvement)
  - <0.4: Poor (system failing)


---
PART 3: EVALUATION PIPELINE
---

How we run evaluation:

1. Create test dataset with:
   - Query
   - Expected answer
   - Relevant documents (ground truth)

2. For each test case:
   - Run query through RAG system
   - Get generated answer + retrieved docs
   - Calculate all metrics

3. Aggregate results:
   - Overall metrics (avg across all queries)
   - Breakdown by category (attendance, syllabus, etc.)
   - Breakdown by difficulty (easy, medium, hard)
   - Identify failures (aggregate score < 0.5)

4. Generate report with:
   - Precision@K, Recall@K, F1@K
   - Semantic similarity, Faithfulness
   - Hallucination rate
   - Detailed results per query
   - Failed examples for analysis


Example evaluation output:

  ğŸ“Š Overall Metrics:
    Precision@5:         0.720
    Recall@5:            0.680
    F1@5:                0.699
    Semantic Similarity: 0.815
    Faithfulness:        0.870
    Hallucination Rate:  12.0%
    Aggregate Score:     0.762

  ğŸ“‚ Category Breakdown:
    attendance: 0.812 (n=3)
    syllabus: 0.745 (n=2)
    exam_rules: 0.698 (n=2)

  âš ï¸ Failed Examples (2):
    - "What is the process to apply for scholarship?" (score: 0.45)


---
PART 4: KEY VIVA POINTS
---

Q: How is this different from just using ChatGPT?

A: Our system:
   1. Uses ONLY university documents (no hallucination from general knowledge)
   2. Provides source citations and confidence scores
   3. Can be evaluated objectively with metrics
   4. Refuses to answer when uncertain


Q: How do you measure accuracy in a RAG system?

A: We use two-stage evaluation:
   1. Retrieval Metrics: Did we find the right documents? (Precision/Recall/MRR)
   2. Generation Metrics: Is the answer correct and grounded? (Semantic Similarity/Faithfulness)
   
   We do NOT claim "LLM accuracy" like classification.
   Our accuracy is based on document retrieval + answer grounding.


Q: What is MMR and why use it?

A: Maximal Marginal Relevance selects diverse results.
   Benefits:
   - Avoids redundancy
   - Shows different aspects of the topic
   - Improves answer completeness


Q: How do you prevent hallucination?

A: Multiple safeguards:
   1. Strict prompt engineering (forces LLM to cite context)
   2. Confidence threshold (refuse if < 0.25)
   3. Faithfulness scoring (detect unsupported statements)
   4. Hallucination detector (flag suspicious sentences)


Q: Can you explain confidence scoring?

A: Confidence combines three factors:
   - Top retrieval score (best match quality)
   - Average retrieval score (overall relevance)
   - Chunk agreement (consistency between retrieved docs)
   
   This tells users how much to trust the answer.


Q: What is faithfulness score?

A: Faithfulness measures if the answer is grounded in retrieved context.
   High faithfulness = answer is supported by documents
   Low faithfulness = possible hallucination
   
   Calculated using semantic similarity between answer and context chunks.


---
PART 5: ARCHITECTURAL DIAGRAM (TEXT)
---

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER QUERY                              â”‚
â”‚               "What is the attendance policy?"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EMBEDDING MODEL                              â”‚
â”‚         (Sentence-Transformers: all-MiniLM-L6-v2)           â”‚
â”‚              Query â†’ Dense Vector (384-dim)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FAISS VECTOR DATABASE                           â”‚
â”‚    Semantic Search: Find top-15 similar document chunks      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MAXIMAL MARGINAL RELEVANCE (MMR)                     â”‚
â”‚   Select 5 diverse chunks (relevant + non-redundant)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONFIDENCE CALCULATOR                           â”‚
â”‚  Score = 0.5Ã—top + 0.3Ã—avg + 0.2Ã—agreement                  â”‚
â”‚              Threshold check: > 0.25?                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                 â”‚
           [Low]â”‚                 â”‚[High]
                â–¼                 â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  REFUSE ANSWER   â”‚  â”‚   BUILD CONTEXT          â”‚
     â”‚  "I could not    â”‚  â”‚   Top 5 chunks +         â”‚
     â”‚   find info..."  â”‚  â”‚   document metadata      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    LLM (GPT-4o-mini)   â”‚
                           â”‚  Enhanced Prompt with: â”‚
                           â”‚  - Context             â”‚
                           â”‚  - Strict instructions â”‚
                           â”‚  - Reasoning demands   â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  HALLUCINATION CHECK   â”‚
                           â”‚  Verify answer âŠ‚       â”‚
                           â”‚  context               â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   FINAL RESPONSE       â”‚
                           â”‚   - Answer             â”‚
                           â”‚   - Sources            â”‚
                           â”‚   - Confidence         â”‚
                           â”‚   - Reasoning          â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---
PART 6: RUNNING THE EVALUATION
---

Command:
  python -m app.evaluation.evaluator

Output:
  - Printed metrics summary
  - JSON file: evaluation_results.json
  - Contains:
    * Overall metrics
    * Category breakdown
    * Difficulty breakdown
    * Detailed per-query results
    * Failed examples

Use this to:
  1. Show system performance objectively
  2. Identify weak areas
  3. Demonstrate improvements over baseline
  4. Justify design decisions in viva


---
END OF VIVA GUIDE
---

This upgraded system is:
âœ“ Smarter (MMR, confidence, validation)
âœ“ Measurable (comprehensive metrics)
âœ“ Explainable (reasoning, transparency)
âœ“ Defensible (objective evaluation)
âœ“ Production-ready (proper error handling, thresholds)
"""
